{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/IF5AeN79EIEJ6ynzWI16"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6QUn6owb8_Mr"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n","        super().__init__()\n","        \n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        \n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        \n","        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","    def forward(self, src):\n","        # src : [sen_len, batch_size]\n","        embedded = self.dropout(self.embedding(src))\n","        \n","        # embedded : [sen_len, batch_size, emb_dim]\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        # outputs = [sen_len, batch_size, hid_dim * n_directions]\n","        # hidden = [n_layers * n_direction, batch_size, hid_dim]\n","        # cell = [n_layers * n_direction, batch_size, hid_dim]\n","        return hidden, cell"]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n","        super().__init__()\n","        \n","        self.output_dim = output_dim\n","        self.emb_dim = emb_dim\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        \n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        \n","        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=self.n_layers, dropout=dropout)\n","        \n","        self.fc_out = nn.Linear(hid_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, input, hidden, cell):\n","        \n","        # input = [batch_size]\n","        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n","        # cell = [n_layers * n_dir, batch_size, hid_dim]\n","        \n","        input = input.unsqueeze(0)\n","        # input : [1, ,batch_size]\n","        \n","        embedded = self.dropout(self.embedding(input))\n","        # embedded = [1, batch_size, emb_dim]\n","        \n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        # output = [seq_len, batch_size, hid_dim * n_dir]\n","        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n","        # cell = [n_layers * n_dir, batch_size, hid_dim]\n","        \n","        # seq_len and n_dir will always be 1 in the decoder\n","        prediction = self.fc_out(output.squeeze(0))\n","        # prediction = [batch_size, output_dim]\n","        return prediction, hidden, cell\n"],"metadata":{"id":"C3KsvL0J-mAh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","        \n","        assert encoder.hid_dim == decoder.hid_dim, \\\n","            'hidden dimensions of encoder and decoder must be equal.'\n","        assert encoder.n_layers == decoder.n_layers, \\\n","            'n_layers of encoder and decoder must be equal.'\n","        \n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        # src = [sen_len, batch_size]\n","        # trg = [sen_len, batch_size]\n","        # teacher_forcing_ratio : the probability to use the teacher forcing.\n","        batch_size = trg.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","        \n","        # tensor to store decoder outputs\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","        \n","        # last hidden state of the encoder is used as the initial hidden state of the decoder\n","        hidden, cell = self.encoder(src)\n","        \n","        # first input to the decoder is the <sos> token.\n","        input = trg[0, :]\n","        for t in range(1, trg_len):\n","            # insert input token embedding, previous hidden and previous cell states \n","            # receive output tensor (predictions) and new hidden and cell states.\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","            \n","            # replace predictions in a tensor holding predictions for each token\n","            outputs[t] = output\n","            \n","            # decide if we are going to use teacher forcing or not.\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            \n","            # get the highest predicted token from our predictions.\n","            top1 = output.argmax(1)\n","            # update input : use ground_truth when teacher_force \n","            input = trg[t] if teacher_force else top1\n","            \n","        return outputs\n"],"metadata":{"id":"BiYxUoCh-qP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First initialize our model.\n","INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","HID_DIM = 512\n","N_LAYERS = 2\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","\n","encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n","decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n","\n","model = Seq2Seq(encoder, decoder, device).to(device)"],"metadata":{"id":"5qWhJdhtBx01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)\n","        \n","model.apply(init_weights)"],"metadata":{"id":"wFRWi3a8DI0Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"metadata":{"id":"3_9Uemw9DKjZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.Adam(model.parameters())\n","\n","TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"],"metadata":{"id":"gfXl5oJDDP3j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        src = batch.src\n","        trg = batch.trg\n","        \n","        optimizer.zero_grad()\n","        # trg = [sen_len, batch_size]\n","        # output = [trg_len, batch_size, output_dim]\n","        output = model(src, trg)\n","        output_dim = output.shape[-1]\n","        \n","        # transfrom our output : slice off the first column, and flatten the output into 2 dim.\n","        output = output[1:].view(-1, output_dim) \n","        trg = trg[1:].view(-1)\n","        # trg = [(trg_len-1) * batch_size]\n","        # output = [(trg_len-1) * batch_size, output_dim]\n","        \n","        loss = criterion(output, trg)\n","        \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"metadata":{"id":"3KYlk6hrDTAu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","        \n","        for i, batch in enumerate(iterator):\n","            \n","            src = batch.src\n","            trg = batch.trg\n","            \n","            output = model(src, trg, 0) # turn off teacher forcing.\n","            \n","            # trg = [sen_len, batch_size]\n","            # output = [sen_len, batch_size, output_dim]\n","            output_dim = output.shape[-1]\n","            \n","            output = output[1:].view(-1, output_dim)\n","            trg = trg[1:].view(-1)\n","            \n","            loss = criterion(output, trg)\n","            \n","            epoch_loss += loss.item()\n","            \n","    return epoch_loss / len(iterator)\n","    \n","# a function that used to tell us how long an epoch takes.\n","def epoch_time(start_time, end_time):\n","    \n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time  / 60)\n","    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n","    return  elapsed_mins, elapsed_secs\n","N_EPOCHS = 10\n","\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iter, criterion)\n","    \n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'Seq2SeqModel.pt')\n","    print(f\"Epoch: {epoch+1:02} | Time {epoch_mins}m {epoch_secs}s\")\n","    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n","    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}\")"],"metadata":{"id":"56oyd2iwDWc_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test():\n","    best_model = Seq2Seq(encoder, decoder, device).to(device)\n","    best_model.load_state_dict(torch.load('Seq2SeqModel.pt'))\n","    \n","    test_loss = evaluate(model, test_iter, criterion)\n","    \n","    print(f\"Test Loss : {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}\")\n","    \n","test()"],"metadata":{"id":"JvKXqOwSDbDZ"},"execution_count":null,"outputs":[]}]}